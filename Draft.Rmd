---
title: "Draft"
author: "Zhi Zhen Qin"
output: 
  word_document
    # reference_docx: C:/Users/zhizh/OneDrive - Stop TB Partnership/UNOPS/10 Paper Writing/CAR software/10 Manuscript/3 Nepal_Cameroon/Deep Learning CXR Compare - Submission SR_ZZ.docx

---

```{r Global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=15, fig.height=18, echo=FALSE, warning=FALSE, message=FALSE)
library(moments)
library(scales)
```
  
# Introduction

The improved theoretical understanding in AI technology, the ubiquity of large annotated datasets, and the advance in computer power have fostered a rapid expansion of the AI industry in medical diagnosis. Until recently, the most accurate AI methods for image analysis required manual segmentation and computation of specific features annotated by experts. Since 2012, deep learning in neural networks is increasingly being used to analyse medical images, such as chest x-rays (CXR). 

CXR is recommended by the World Health Organization (WHO) as a screening and triage tool for tuberculosis (TB), a disease which kills more people than any single infectious disease world-wide. However, the use of CXR is limited due to high inter-and intra-reader variability and low specificity. Additionally, there is a lack of qualified radiologists who can quickly read CXR films in high TB burden countries. AI technology presents a promising solution to overcome these obstacles. Such technology makes use of neural networks and deep learning to identify TB-related abnormalities on chest radiographs. Inspired by the human nervous system, neural networks are interconnected functions, each comprised of a weight and a bias coefficient. In deep learning, the networks are trained using large sets of known positive and negative data (ground truth) in multiple hidden layers. The networks “learn” by adjusting the weights and biases of the underlying functions based on the difference between predictions and ground truth, a process called back-propagation. A complex deep learning network can modify and ‘train’ itself using a large training dataset, enabling it to identify new and unseen data. 

How exactly AI algorithms detect TB abnormality is unclear, even to those who developed them, and how the algorithms were constructed is a fiercely guarded trade secret, earning AI the “black box” reputation. Furthermore, often the marketed accuracy of AI software is done on the same data superset for training, testing and validation and cannot be generalized to other settings.

Several AI companies have emerged in recent years promising to quickly screen digital chest-radiographs to identify people in need of further confirmation testing for TB. However, current scientific evidence is limited and mostly available for an earlier version of one product, CAD4TB (Delft Imaging Systems, Netherlands). There is one peer-reviewed publication on the performance of other AI software for detecting TB abnormalities with a relatively small dataset. WHO has not made a recommendation on the use of automated reading systems for TB due to the current lack of evidence [WHO]. In response to this, we evaluated multiple AI software available for TB screening and triage using a large dataset unseen by any AI developers to help National TB Programmes, medical professionals and patients to assess the true diagnostics accuracy of these algorithms.


# Methods
In this retrospective study, we used data consecutively from all adults (> 15 years), presented in any of the three TB Screening Centres established by icddr,b, with funding from the Stop TB Partnership’s TB REACH Initiative, between 15 May 2014 and 4 October 2016. The three TB centres cover a vast network of more than 2,000 private providers and 133 NTP facilities across Dhaka. After providing informed consent, each participant was verbally screened for symptoms and received a posterior-anterior CXR using digital X-ray machines (Delft EZ DR X-ray) and was asked to submit a sputum sample for a free Xpert test. Xpert test was performed again if the initial test failed (invalid, error, or no result). The final Xpert results were used as the bacteriological evidence and the reference standard in this evaluation. A group of 3 Bangladeshi, board-certified radiologists (all with MD/MBSS degree in radiology and X,Y,Z  years of experience) read all of the CXR images remotely. The radiologists were blinded to all testing and clinical (and demographic data ) data and provided standard radiology reports. They graded each CXR as follows (criteria to be attache d): a. Highly Suggestive of TB: including highly suggestive of TB only, b. Possibly TB: including abnormality highly suggestive of TB and abnormality possibly associated with TB, c. Any Abnormality: including abnormality highly suggestive of TB, possibly associated with TB and non-TB abnormality, d. Normal. Anyone who was unable or unwilling to acquire a CXR was referred to a nearby public-sector health facility.

We identified five DL systems with stable version control through the network and the database of innovators developed under the TB REACH initiatives and the Accelerator for Impact (a4i) project at Stop TB Partnership.    to  include in this study: CAD4TB (v6), qXR (v2) developed by Qure.ai (India), Lunit INSIGHT for Chest Radiography (v4.7.2) developed by Lunit (South Korea), JF CXR-1 (v2) developed by JF Healthcare (China) and InferRead®DR (v2) by Infervision (China). The 5 DL systems scored the images remotely through Secured File Transfer Protocol from the Stop TB repository except for Delft, in which case the data was shared through cloud transfer. All machine reading was performed independently, with the developers blinded to all testing, clinical and demographic data. All DL systems produce a continuous abnormality score (from 0 to 100 or from 0% to 100%) which presents/represents the probability of presence of TB.



## Data Analysis 
We first compared the performance of the group of Bangladeshi radiologists and the 5 DL systems, using bacteriological results as the ground truth. We calculated the sensitivity and specificity of the following radiologists’ reading, 1) Highly Suggestive of TB, 2) Possibly TB: , and 3) Any Abnormality. The threshold scores of the DL systems were chosen to produce the same dichotomized decisions as the radiologists in terms of sensitivity for each human reading category. We compared the difference in specificity of the human reading and of the predictions from the 5 DL systems using McNemar test for paired proportions.

We compared the overall performance of the 5 DL systems using the area under receiver operating characteristic (ROC) curves (AUC), which shows the tradeoff between sensitivity and specificity with varying thresholds. However, since the occurrence of TB and not TB is imbalanced, we also calculated the area under Precision-Recall (PRC) curve (PRAUC), which shows precision values for corresponding sensitivity values and is more informative than the ROC curve when evaluating binary classifier on imbalanced datasets [Saito 2015].

We proposed a new analytic framework to evaluate screening and triage tests with continuous output and understand threshold selection by factoring in the ability of reduce subsequent confirmation testing which is measured by 1) the proportion of subsequent Xpert saved where 0% saved represents everyone receive a Xpert test regardless of CXR results and 2) the number of subsequent Xpert needed to test (NNT) to find one Bac+ patient. We plotted the sensitivity against the proportion of Xpert saved to show the tradeoff between finding as many Bac+ patients as possible and the cost saving of each DL systems. Additionally, to help our understanding in threshold selection, we plotted the dynamic among sensitivity, proportion of Xpert saved and NNT was visualized with varying decision threshold in a three-way plot. We then assess the distribution of abnormality scores disaggregated by Xpert status and by prior history of TB. Mann-Whitney U test was used to compared non-normal distribution.

Finally, we divided the test population by patient age, prior TB history and referral types to evaluate the performance of the 5 DL systems in each subpopulation in terms of AUC.

## Ethics
All enrolled participants provided informed written consent. The study protocol was reviewed and approved by the Research Review Committee and the Ethical Review Committee at the International Centre for Diarrhoeal Disease Research, Bangladesh (icddr,b).

## Role of the AI Developers
The AI developers had no role in study design, data collection, analysis plan, or writing of the study. The developers only had access to the CXR images, and did not receive any of the demographic, symptom, medical, or testing data of the participants.


# Results

```{r Load, warning=FALSE, results='hide'}
source("Chapter/table1.R")
```
  
A total of 24,031 people were consecutively recruited from the three TB SCscentres. Excluding 17 individuals without a CXR and 107 individuals aged 15 or less, a total of `r sum(MDF$Age>=15)`  individuals were included in this analysis. The median age was `r T1$Overall[2]`, and `r paste0(substr(as.character(T1$Overall[[3]]), 13,16), "%")` were female, and almost all (`r paste0(substr(as.character(T1$Overall[[9]]), 8,11), "%")`) had at least one TB-related symptom. The most common symptoms are cough (`r paste0(substr(as.character(T1$Overall[[4]]), 8,11), "%")`), fever (`r paste0(substr(as.character(T1$Overall[[5]]), 8,11), "%")`), weight loss  (`r paste0(substr(as.character(T1$Overall[[7]]), 8,11), "%")`), and shorteness of breath (`r paste0(substr(as.character(T1$Overall[[6]]), 8,11), "%")`). Hemoptysis was only reported in `r paste0(substr(as.character(T1$Overall[[8]]), 8,10), "%")` participants. Noticeably, there were `r paste0(substr(as.character(T1$Overall[[10]]), 7,10), "%")` participants suspected of having TB already had prior TB history. The prevalence of bacteriologically-confirmed (Bac+) TB, confirmed by Xpert, in this population overall was `r paste0(substr(as.character(T1$Overall[[11]]), 7,10), "%")`. Among Bac+ individuals, `r percent(sum(MDF$RIF.Result %in% "Detected")/sum(MDF$Xpert2Outcome_num %in% "1"))` were resistant to rifampicin. The radiologists graded `r T1$Overall[25]` as Highly Suggestive, `r T1$Overall[26]` as Possibly TB, `r T1$Overall[27]` as Any Abnormality. The medians of the abnormality scores in Bac+ group were significantly different from those in the Xpert negative group were (Mann-Whitney U test <0.05) for all 5 DL systems. 


Three general types of patients visited the TB screening and test centre. The greatest proportion of test results came from people who were referred by either public sector or private sector health providers (n=`r T1$Overall[22]`);  `r T1$Overall[23]` individuals were tested in NTP DOTS facilities after an inital negative smear; and `r T1$Overall[24]` individuals walked-in / self-presented (Table 1). Across the three groups, the prevalence of Xpert positive TB was lowest among walk-ins: `r T1$WalkIn[11]`, highest in the NTP DOTS retested subgroup (`r T1[11,6]`).   
  

### Comparison of sensitivity and specificity between human reading and the predictions of the DL systems
```{r}
source("radiologist.R")
humanAI <- read.csv("Results/HumanAI.csv", )
knitr::kable(humanAI, row.names = F)
```

To compare the DL systems withthe group of Bangladeshi radiologists, we choose thresholds to match the sensitivity of each grading made by the human readers. The grading of "highly suggestive of TB" had a low sensitivity of `r paste0(percent(Radiologist$Sens[1]), " (95%CI: ", percent(Radiologist$Sens_L[1]), " - ", percent(Radiologist$Sens_H[1]),")")` but a high specificity of `r paste0(percent(Radiologist$Spec[1]), " (95%CI: ", percent(Radiologist$Spec_L[1]), " - ", percent(Radiologist$Spec_H[1]),")")`. All 5 DL systems demonstrated statistically significant improvement in specificity. The increases in specificities range from 1.48% (CAD4TB) to 8.96% (Lunit INSIGHT CXR).     

If radiologists' rading of "Possibly TB" had been used to triage the follow-on Xpert testing, the resulting sensitivity would be `r paste0(percent(Radiologist$Sens[2]), " (95%CI: ", percent(Radiologist$Sens_L[2]), " - ", percent(Radiologist$Sens_H[2]), ")")` and the specificity would be `r paste0(percent(Radiologist$Spec[2]), " (95%CI: ", percent(Radiologist$Spec_L[2]), " - ", percent(Radiologist$Spec_H[2]), ")")`. The matching AI classificiation by the 5 DL systems continued showing statistical improvement in specificty. The increases in specificities range from 1.34% (JF CXR-1) to 7.62% (Lunit INSIGHT CXR). 

The grading of "any abnormality" has high sensitivity, `r paste0(percent(Radiologist$Sens[3]), " (95%CI: ", percent(Radiologist$Sens_L[3]), " - ", percent(Radiologist$Sens_H[3]), ")")`, but a reduced specificity, `r paste0(percent(Radiologist$Spec[3]), " (95%CI: ", percent(Radiologist$Spec_L[3]), " - ", percent(Radiologist$Spec_H[3]), ")")`. The matching AI classification statistically outperform in terms of specificity. qXR gained the highest in specificity (`r paste0(humanAI$Diff.specificity[12], " (95%CI: ", humanAI$CI[12], ")")`).  


###  ROC & PRC

![](Results/5 Curves.tif){width=900px}

```{r Overall performance}
source("Chapter/roc.R")

Supp_Tab <- read_csv("Results/Supp Tab.csv")
Supp_Tab$Score[Supp_Tab$DeepLearningSystem %in% "CAD4TB"] <- Supp_Tab$Score[Supp_Tab$DeepLearningSystem %in% "CAD4TB"]/100
Supp_Tab <- subset(Supp_Tab, Supp_Tab$Site %in% "BGD")
# View(Supp_Tab)
Supp_Tab <- Supp_Tab[Supp_Tab$Score %in% c(0.6, 0.7, 0.8), c(2:4, 8, 9)]
knitr::kable(Supp_Tab)
```


The tradeoffs between sensitivity and specificity of the 5 DL systems can be visualized in the ROC  (Figure xxx) and Precision-Recall (Figure xxx) graphs. The AUCs under the ROC curve from high to low are Lunit INSIGHT CXR: 0.89 (95% CI:0.88-0.89), qXR: 0.87 (95% CI:0.87-0.88), InferReadDR: 0.85 (95% CI:0.84-0.86), JF CXR-1: 0.85 (95% CI:0.84-0.85), CAD4TB: 0.82 (95% CI:0.82-0.83).  The differneces in AUCs of Lunit INSIGHT, qXR and CAD4TB are statistically significant, but the difference between JF CXR-1 and InferReadDR is not statistically significant. The ROC curves and precision recall curves of InferReadDR and JF CXR-1 are almost completely overlap. 

Regarding the trade off between sensitivity and ability to triage, figure x shows that all 5 DL systems can at least half follow-on testing while still keep a high sensitivity above 90%. However, as more follow-on tests are triaged (especially >60%), the 5 DL systems demonstrated difference in sensitivity. For instance, when the follow-on testing is reduced by 2/3 (blue dotted line), the sensitivity is between 85%-77% with Lunit INSIGHT CXR and qXR have the highest sensitivity, followed by JF CXR-1 and InferReadDR and CAD4TB. 

Atlhough there is no difference between InferReadDR and JF CXR-1 in terms of ROC and PR curves, Figure xxx -xxx, show the two tests have distinct performance: for most of the decision thresholds (approximately above 0.15), JF CXR-1 has a higher sensitivity, but less ability to save follow-on testing ability and required higher NNT than InferReadDR. For instance, at 0.8 cutoff theshold, JF CXR-1 is `r Supp_Tab[12,3]` sensitive, can save `r (Supp_Tab[12,5])` of follow-on Xpert testing, and has a NNT of `r Supp_Tab[12,4]`. At the same decision threshold however, InferReadDR has a much lower sensitivity, `r Supp_Tab[9,3]`, but can significantly triage (`r (Supp_Tab[9,5])`) follow-on testing and the required NNT is reduced to `r Supp_Tab[9,4]`. 

It is useful thresholding technique to examine the performance -- sensitivity, saving of following-on testing, NNT -- with across varying cutoff points to better dichotomize probability scores to a TB/not TB binary category. We observed that as cutoff threshold increases from 0, the sensitivity of CAD4TB only slowly decreases to 95%, the proportion of Xpert saved increased to 30% and NNT slowly descreases to 5.5 as threshold point reaches 0.5; however a further increases in threshold point from 0.5 results in a sharp descrease in sensitivity and NNT and increases in Xpert saved. We also observe that the sensitivity and NNT of JF CXR-1 remain above 90% and 3 respectively for the most of threshold scores (between 0-0.9). The sensitivity and NNT of Lunit and qXR remain above around 80% before quickly decreases. 

From Figure x, it is clear that there is no decision threshold that is universally applicable to all DL products. For example, the threshold to achevie at least 90% sensitivity for InferReadDR must be below 0.35, Lunit 0.59, CAD4TB and qXR 0.62, and JF CXR-1 anywhere below 0.93. 


![](Results/ROC_AUC plot.tif){width=900px}

We compared the performance of the 5 DL systems across age groups, sources of patient and prior TB history. We find that all 5 DL systems perform worse in old age (above 65 years old) than in yong age (15-25 years old) and middle age (25-65 years old). The difference in the DL systems' performance in people with and without prior TB history is profound. We hypothesize that the abnormalities on the chest due to age and prior TB hisotry influenced the classification of active TB. 

### Density plot 
![](Results/density.tif){width=900px}

The stacked density plot in Fig-X shows the distributions of the 5 DL systems’ scores disaggregated by Xpert outcomes and by prior TB history. The dark and light red bars were Bac negative and the dark and light green bars were Bac positive. The distributions of the 5 DL systems differ, indicating the underlying models are different. An ideal test should have a density plot with all the red bars left skewed (positive skew) and all green bars right skewed with no overlap. Lunit’s, qXR’s and InferReadDR’s density plot demonstrated this dichotomization pattern. Although almost all Bac positive patients received high abnormality scores (95-100) from JF CXR-1, so were many Bac negative patients, of which, mostly were healthy people but with a prior TB history (dark red), which can lead to excessive recall. When separately examine the distribution of the abnormality scores from the health people but with prior TB history, none of the DL systems has a left skewed distribution (InferReadDR: `r round(skewness(MDF[MDF$Xpert2Outcome_num ==0 & MDF$TB.Medication.History %in% "Yes", 33]),3)`,  qXR: `r round(skewness(MDF[MDF$Xpert2Outcome_num ==0 & MDF$TB.Medication.History %in% "Yes", 28]),3)`, Lunit INSIGHT CXR: `r round(skewness(MDF[MDF$Xpert2Outcome_num ==0 & MDF$TB.Medication.History %in% "Yes", 29]),3)`, CAD4TB: `r round(skewness(MDF[MDF$Xpert2Outcome_num ==0 & MDF$TB.Medication.History %in% "Yes", 27]),3)`, and JF CXR-1: `r round(skewness(MDF[MDF$Xpert2Outcome_num ==0 & MDF$TB.Medication.History %in% "Yes", 30]),3)`), which we postulated resulted from issues in the algorithms' ability to differentiate between old scarrings and active lesions.

# Discussion

To our knowledge, this is the largest independent study of multiple AI algorithms for detecting TB abnormalities in CXR and the first published evaluation of JF CXR-1 and InferReadDR for detecting TB. 
  

This is the first evaluation of multiple DL systems for detecting TB abnormalities in CXR. We observed that all three systems performed significantly better than human radiologists and had higher AUCs than most of the current published literature on previous versions of CAD4TB 6. Our results also document the first published evaluation of qXR and Lunit for detecting TB. There was no statistical difference among the AUCs of CAD4TB, Lunit, and qXR across the study sites, in pooled analysis, and when only smear negative individuals were considered. The point estimate for CAD4TB met the TPP target for a community-based triage test. However, there was no statistical difference between the specificity of CAD4TB and Lunit at the sensitivity level of 95% and a marginal difference with qXR. At the 94% or 96% sensitivity point estimate, none of the three DL systems were significantly different. These results did not hold at the site level where there was no statistically significant difference. The overall performance of the three DL systems was similar in multiple analyses and stratifications. Implementers considering using DL systems for CXR reading should take into account other factors including service, ease of use, maintenance and price - all important considerations in any new technology implementation 25. 

While the current WHO guidance for these computer-aided detection software for TB emphasizes the need of predefined threshold scores 8, our results clearly indicate the need for implementers  to conduct their own pilot on the specific population being tested. A previous study found differences in performance by age and by referral site 13. With large datasets, it may be possible to tailor specific thresholds depending on the characteristics of individuals screened. In some published literature, specific threshold scores for different versions of CAD4TB have been used 30 and following these scores or manufactures default settings may produce different results across settings.

There are a number of limitations in our study. Due to logistic and budgetary constraints, we did not use culture as the reference standard. We were not able to obtain the HIV status of some of the participants in both sites, limiting our ability to analyze the products in this population. Lastly, since we retrospectively collected and analyzed the radiographs in the sites where CAD4TB was implemented, the radiographs have been seen by CAD4TB. However, neither qXR nor Lunit had seen the images prior to this study. 
